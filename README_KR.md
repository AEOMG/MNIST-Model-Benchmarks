#### 서론
이 코드들은 MNIST 데이터셋을 사용하여 각 모델이 어떻게 작동하는지 파악할 수 있도록 여러 가지 방법을 사용한 코드들입니다. 이 프로젝트는 다양한 모델을 사용하여 MNIST 데이터셋을 학습하고 평가하며, 각 모델의 구조와 결과를 분석할 수 있도록 돕습니다.

#### **MNIST 데이터셋에 대한 설명**
MNIST(Mixed National Institute of Standards and Technology) 데이터셋은 손글씨 숫자 인식 문제를 다루기 위해 널리 사용되는 **기본적인 이미지 데이터셋**입니다. 이 데이터셋은 **0부터 9까지의 숫자**가 손으로 쓰여진 이미지들로 구성되어 있으며, 총 **70,000개(훈련용 60,000개, 테스트용 10,000개)**의 샘플로 이루어져 있습니다.

- **이미지 크기**: 각 이미지는 **28x28 픽셀**의 크기를 가지며, 이를 **784개의 픽셀 값**으로 표현할 수 있습니다.
- **레이블**: 각 이미지는 **0부터 9 사이의 레이블**로 분류됩니다.
- **데이터 형식**: 픽셀 값은 **0부터 255**까지의 정수로 저장되어 있어, 각 픽셀의 밝기를 나타냅니다. 이 데이터는 `.gz` 포맷으로 제공되어 있으며, 압축을 풀어야 사용할 수 있습니다.

MNIST 데이터셋은 다양한 딥러닝 모델의 **성능을 평가**하거나 **기본적인 학습 프로세스를 이해**하기 위한 **기본적인 데이터셋**으로 많이 활용됩니다. 특히 초보자들이 **신경망의 개념**을 배우고 실습하기에 적합합니다.

#### **전처리 과정 설명과 이해**
전처리 과정은 데이터셋을 **모델이 학습하기에 적합한 형태로 변환**하는 단계입니다. MNIST 데이터셋의 경우, 전처리는 모델의 학습 성능을 극대화하고 학습을 효율적으로 진행하기 위해 매우 중요합니다. 전처리 과정은 다음의 주요 단계로 나눌 수 있습니다.

1. **데이터 로드**
   - MNIST 데이터셋은 `.gz`로 압축된 **바이너리 파일**로 제공되기 때문에 이를 파싱하는 작업이 필요합니다.
   - 데이터 로드 단계에서는 이미지를 **28x28 픽셀**의 행렬로 로드하고, 이후 모델 입력을 위해 **1차원 벡터(784 차원)**로 변환합니다.
   - **레이블 데이터**도 함께 로드되어 각 이미지가 어떤 숫자인지 나타내는 정보를 제공하게 됩니다.

2. **정규화 (Normalization)**
   - **정규화**는 데이터의 각 특성 값을 **[0, 1]**의 범위로 변환하는 과정입니다. MNIST의 경우, 각 픽셀 값이 **0부터 255** 사이의 정수 값으로 표현되므로, 이를 **255.0으로 나누어** **0~1 사이의 실수**로 변환합니다.
   - **정규화의 목적**은 학습 과정에서 **수렴 속도를 높이고** 신경망의 **안정적인 학습**을 가능하게 하기 위함입니다. 정규화를 통해 입력 값의 범위를 일정하게 맞추면, 가중치가 조정되는 과정에서 보다 효율적인 학습이 가능합니다.

3. **원-핫 인코딩 (One-Hot Encoding)**
   - **레이블 데이터**는 0부터 9까지의 정수로 되어 있으므로, 이를 **신경망 출력층에 적합한 형태**로 변환해야 합니다. 이때 사용하는 기법이 **원-핫 인코딩**입니다.
   - 원-핫 인코딩은 각 레이블을 길이 10의 벡터로 표현하며, 해당 클래스에 해당하는 위치에만 `1`의 값을 갖고, 나머지 값은 `0`이 됩니다.
     - 예를 들어, 레이블 `3`은 `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`으로 변환됩니다.
   - 이를 통해 **출력층에서 각 숫자에 대한 확률**을 계산하고, 예측할 때 **가장 높은 확률을 가진 클래스**를 선택할 수 있습니다.

4. **데이터셋 분할**
   - 전체 MNIST 데이터셋은 **훈련 데이터**와 **테스트 데이터**로 나뉩니다.
   - **훈련 데이터**는 모델 학습에 사용되며, **테스트 데이터**는 학습된 모델의 성능을 평가하는 데 사용됩니다.
   - 데이터셋의 이러한 분할은 **일반화 성능**을 확인하는 데 중요하며, 학습 과정 중 데이터에 과적합되지 않도록 도와줍니다.

#### **모델 설명**
이제 MNIST 데이터셋을 사용한 여러 가지 방법에 대해 알아보겠습니다. 이 코드들에서는 **tanh**, **SGD**, **ReLU+Momentum**, **Adam**, **AdamW**와 같은 방법이 사용되었으며, 각 모델이 어떤 구조를 가지는지 파악할 수 있고, 어떤 결과가 나타나는지 알 수 있습니다.

1. **Tanh 활성화 함수**
   - **Tanh**는 하이퍼볼릭 탄젠트 함수로, 입력 값을 **-1에서 1** 사이로 변환합니다. 이 함수는 데이터를 0을 중심으로 정규화하여 학습 속도를 향상시키는 데 도움이 될 수 있습니다.
   - **MNIST 데이터셋**에 적용할 때, tanh는 은닉층에서 효과적일 수 있지만, 특히 깊은 신경망에서는 **기울기 소실 문제**가 발생할 수 있어 학습이 어려워질 수 있습니다.

2. **SGD (Stochastic Gradient Descent)**
   - **SGD**는 각 훈련 예제를 기반으로 가중치를 업데이트하는 경사 하강법의 일종으로, 전체 데이터셋 대신 개별 예제에 대해 업데이트를 수행하여 메모리 사용량을 줄일 수 있습니다.
   - **MNIST 데이터셋**에 적용하면, 데이터셋이 비교적 작을 때 모델을 점진적으로 최적화할 수 있어 빠른 수렴을 도울 수 있습니다.

3. **ReLU + Momentum**
   - **ReLU (Rectified Linear Unit)**는 양의 입력 값을 그대로 전달하고 음의 입력 값을 0으로 설정하여 **기울기 소실 문제**를 피하도록 돕습니다.
   - **Momentum**은 이전 가중치 업데이트의 일부를 현재 업데이트에 추가하여 경사면이 급한 영역에서의 **진동을 줄이고 수렴 속도를 높입니다**.
   - **ReLU**와 **Momentum**을 **MNIST 데이터셋**에 적용하면, 모델이 더 깊은 네트워크에서도 **빠르고 안정적인 학습**을 수행할 수 있습니다.

4. **Adam 옵티마이저**
   - **Adam**은 Adaptive Moment Estimation의 약자로, **모멘텀**과 **RMSProp**의 장점을 결합하여 각 파라미터에 대해 **적응형 학습률**을 사용합니다. 이를 통해 효율적이고 빠른 수렴이 가능합니다.
   - **MNIST 데이터셋**에 적용할 때, **Adam**은 **효율적인 학습**을 가능하게 하며, 하이퍼파라미터 튜닝이 많이 필요하지 않아 일반적으로 좋은 결과를 제공합니다.

5. **AdamW 옵티마이저**
   - **AdamW**는 Adam 옵티마이저의 변형으로, **가중치 감소(L2 정규화)**와 기울기 기반 파라미터 업데이트를 분리합니다. 이를 통해 **가중치 감소 메커니즘**을 더 잘 제어할 수 있어 모델의 **일반화 성능**을 향상시키는 데 도움이 됩니다.
   - **AdamW**를 **MNIST 데이터셋**에 적용하면, **안정적인 학습**을 돕고 가중치가 지나치게 커지지 않도록 하여 더 나은 모델 성능을 얻을 수 있습니다.

각 모델은 **고유한 학습 전략**을 사용하여 MNIST 데이터셋의 학습 과정과 성능에 어떤 영향을 미치는지 관찰할 수 있습니다. 이러한 다양한 접근 방식을 사용하여, 각 알고리즘이 학습 속도와 모델의 전체 정확도에 미치는 영향을 이해할 수 있습니다.

## 설치 방법

### 1. Git 클론

이 프로젝트를 로컬에 복사하려면 다음 명령어를 사용하여 GitHub 리포지토리를 클론합니다.

```bash
git clone https://github.com/AEOMG/MNIST-Model-Benchmarks
cd MNIST
```

### 2. Python 환경 설정 및 패키지 설치

이 프로그램은 Python 3.12 버전과 여러 라이브러리를 사용합니다 다음 명령어를 이용해 라이브러리를 설치해주세요.

```bash
pip install -r requirements.txt
```

**주의** : 현재 상대경로로 설정되어 있습니다. 작동되지 않을경우 절대 경로로 바꿔주세요.